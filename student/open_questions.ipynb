{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8e74a689",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Open Questions\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1957f",
   "metadata": {},
   "source": [
    "## 1) Training Dynamics (MLP vs LSTM)\n",
    "\n",
    "From the MLP curves, training improves steadily and validation improves too, but there is a small generalization gap by the end:\n",
    "\n",
    "- Train loss keeps decreasing while val loss flattens, which is a classic mild overfitting pattern.\n",
    "- Train accuracy/F1 remain a bit higher than validation.\n",
    "\n",
    "\n",
    "![](outputs/mlp/mlp_loss.png)\n",
    "![](outputs/mlp/mlp_acc.png)\n",
    "![](outputs/mlp/mlp_f1.png)\n",
    "\n",
    "Some changes might reduce overfitting:\n",
    "\n",
    "- Increase regularization: slightly higher dropout or stronger weight decay.\n",
    "- Add early stopping (stop when val macro-F1 stops improving).\n",
    "- Reduce hidden size a bit, smaller model = less capacity to overfit.\n",
    "\n",
    "\n",
    "\n",
    "The LSTM shows stronger overfitting:\n",
    "\n",
    "- Training loss falls sharply to very low values, but validation loss stops improving and then increases noticeably after about mid training.\n",
    "- Train accuracy/macro-F1 rise very high, while validation levels off much lower.\n",
    "\n",
    "This indicates the LSTM is learning patterns that fit training well but do not generalize as well.\n",
    "\n",
    "\n",
    "![](outputs/lstm/lstm_loss.png)\n",
    "![](outputs/lstm/lstm_acc.png)\n",
    "![](outputs/lstm/lstm_f1.png)\n",
    "\n",
    "Some change to address LSTM overfitting:\n",
    "\n",
    "- Use early stopping, save best by val macro-F1 and stop once it hasn’t improved for a few epochs.\n",
    "- Increase dropout (especially on the classification head; optionally increase LSTM dropout by using `num_layers > 1`).\n",
    "- Reduce model capacity (smaller hidden size, or remove bidirectionality if used).\n",
    "- Stronger weight decay / slightly smaller learning rate can also help.\n",
    "\n",
    "\n",
    "When useing the class weights, in the confusion matrices, both models learn to predict *neg* and *pos* rather than collapsing into mostly *neu*. The LSTM especially increases correct *pos* and *neg* predictions compared to the MLP.\n",
    "\n",
    "Weighted loss can make updates “larger” for minority-class mistakes, which often causes the validation curves to look more jagged early on, but the final macro-F1 is usually better because the model is not ignoring minority classes.\n",
    "\n",
    "\n",
    "![](outputs/mlp/mlp_confusion_matrix.png)\n",
    "![](outputs/lstm/lstm_confusion_matrix.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144fd5f",
   "metadata": {},
   "source": [
    "## 2) Model Performance and Error Analysis (MLP vs LSTM)\n",
    "\n",
    "\n",
    "The LSTM generalized better than the MLP based on the test confusion matrices (and the metrics derived from them):\n",
    "\n",
    "- MLP test accuracy: \\( approx 0.715\\)  \n",
    "- LSTM test accuracy: \\( approx 0.751\\)\n",
    "\n",
    "- MLP test macro-F1 (from confusion matrix) \n",
    "- LSTM test macro-F1 (from confusion matrix)\n",
    "\n",
    "So the LSTM has higher test accuracy and higher test macro-F1, which is stronger evidence of better generalization (macro-F1 is especially important for imbalanced classes).\n",
    "\n",
    "\n",
    "![](outputs/mlp/mlp_confusion_matrix.png)  \n",
    "![](outputs/lstm/lstm_confusion_matrix.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "By raw count (most frequent overall): Neutral (neu) is misclassified the most in both models because it is the largest class and sits “between” negative and positive:\n",
    "\n",
    "- MLP: neu misclassified 97 times  \n",
    "- LSTM: neu misclassified 94 times\n",
    "\n",
    "By error rate : Positive (pos) is misclassified the most:\n",
    "\n",
    "- MLP pos error rate: \\( approx 41.7\\% \\)\n",
    "- LSTM pos error rate: \\( approx 32.4\\% \\)\n",
    "\n",
    "\n",
    "\n",
    "Likely reason for this to happen: Neutral is semantically close to both neg and pos, so many sentences have subtle wording where sentiment is weak/implicit → the model confuses “slightly positive/negative” with neutral. And financial text is often phrased cautiously, with limited emotional words. Small cues like “expected”, “may”, “forecast”, “pressure”, “improve” can flip sentiment but are easy to miss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd9498",
   "metadata": {},
   "source": [
    "## 3) Cross-Model Comparison (MLP, RNN, LSTM, GRU, BERT, GPT)\n",
    "\n",
    "\n",
    "The MLP uses mean-pooled FastText, which collapses each sentence into a single 300-d vector by averaging word vectors. This limits it because there is no word order, like sentence \"profits fell despite strong guidance” vs “strong guidance despite profits fell” can become very similar after averaging. And ther is no phrase structure, negation/modifiers like “not”, “barely”, “despite”, “however” get diluted in the average. In addition, there is no token-level emphasis, one crucial sentiment word can be washed out by many neutral words.\n",
    "\n",
    "\n",
    "\n",
    "The LSTM is a sequence mode: it processes a fixed-length sequence in order, and the final hidden state summarizes the sentence.\n",
    "So the advantages compare to MLP are\n",
    "\n",
    "- Models word order.\n",
    "- Captures patterns like negation, contrast, and clause-level shifts.\n",
    "- Uses context: the meaning of a word can depend on surrounding words.\n",
    "\n",
    "\n",
    "Did fine-tuned LLMs (BERT/GPT) outperform classical baselines?\n",
    "\n",
    "Yes. BERT and GPT outperform the FastText+neural baselines because they start from pretrained contextual representations:\n",
    "- Pretraining on massive corpora teaches general language patterns, syntax, semantics, and domain-relevant associations.\n",
    "- Their embeddings are contextual: the representation of “rise”, “cut”, “beat”, “miss” changes based on surrounding words.\n",
    "- Fine-tuning adapts those rich features to sentiment classification with relatively little labeled data.\n",
    "\n",
    "**BERT**  \n",
    "![](outputs/bert/bert_f1_learning_curves.png)  \n",
    "![](outputs/bert/bert_confusion_matrix.png)\n",
    "\n",
    "**GPT**  \n",
    "![](outputs/gpt/gpt_f1_learning_curves.png)  \n",
    "![](outputs/gpt/gpt_confusion_matrix.png)\n",
    "\n",
    "\n",
    "\n",
    "Rank all six models\n",
    "\n",
    "Using the test confusion matrices, the test macro-F1 ranking is:\n",
    "\n",
    "1. **BERT** \n",
    "2. **GPT** \n",
    "3. **LSTM** \n",
    "4. **GRU**  \n",
    "5. **MLP** \n",
    "\n",
    "Why this ranking makes sense:\n",
    "- BERT/GPT : pretrained contextual features + transformer attention → strongest generalization and best handling of subtle sentiment cues.\n",
    "- LSTM/GRU: sequence modeling captures order/negation/phrases; GRU and LSTM are similar, with small differences depending on hyperparams and regularization.\n",
    "- MLP: mean pooling removes order and weakens compositional meaning → struggles more on nuanced examples.\n",
    "- Vanilla RNN: weaker at long-range dependencies (vanishing gradients) compared to gated models (LSTM/GRU), so it tends to confuse sentiment when important cues occur later in the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e8581",
   "metadata": {},
   "source": [
    "## AI Use Disclosure\n",
    "\n",
    "\n",
    "- **Tool(s) used:** Chatgpt\n",
    "- **How you used them:** I give it the code I wrote and ask if it is correct, I also asked it of the errors I got and the suggestion to fix them. And I used it to refine my answer to the open queation\n",
    "- **What you verified yourself:** I checked the output to see whether it is reasonable\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
